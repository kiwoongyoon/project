{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-06T16:53:20.375591Z","iopub.status.busy":"2024-04-06T16:53:20.374904Z","iopub.status.idle":"2024-04-06T16:53:20.443232Z","shell.execute_reply":"2024-04-06T16:53:20.442163Z","shell.execute_reply.started":"2024-04-06T16:53:20.375560Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e5d2f7a04ea44b4a4a3f5810097ee36","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acf37de1e10a400aa0f41922b4d3e950","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4515447c93a427a96f172081009da17","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2a66e71d7c045a594b33a67b45695dd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"data":{"text/plain":["'cuda'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","import numpy as np \n","import pandas as pd \n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import os \n","# 텐서로 변환\n","transform = transforms.Compose([     transforms.ToTensor()])\n","# 훈련 데이터셋\n","train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","# 테스트 데이터셋\n","test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","        \n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{},"source":["# Seed 고정하기"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:53:20.445876Z","iopub.status.busy":"2024-04-06T16:53:20.445413Z","iopub.status.idle":"2024-04-06T16:53:20.452403Z","shell.execute_reply":"2024-04-06T16:53:20.451361Z","shell.execute_reply.started":"2024-04-06T16:53:20.445839Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(42) # Seed 고정"]},{"cell_type":"markdown","metadata":{},"source":["# 하이퍼 파라미터 정의하기"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:53:39.730086Z","iopub.status.busy":"2024-04-06T16:53:39.729290Z","iopub.status.idle":"2024-04-06T16:53:39.742015Z","shell.execute_reply":"2024-04-06T16:53:39.741085Z","shell.execute_reply.started":"2024-04-06T16:53:39.730053Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.0012\n","training_epochs = 20\n","BATCHSIZE = 64\n","\n","train_dataset_size = int(len(train) * 0.9)\n","validation_dataset_size = int(len(train) * 0.1)\n","train_dataset, validation_dataset = random_split(train, [train_dataset_size, validation_dataset_size])\n","train_dataset_loader = DataLoader(dataset=train_dataset, batch_size=BATCHSIZE, shuffle=True)\n","validation_dataset_loader = DataLoader(dataset=validation_dataset, batch_size=BATCHSIZE, shuffle=True)\n","test_dataset_loader = DataLoader(dataset=test, batch_size=BATCHSIZE, shuffle=True)\n","     \n"]},{"cell_type":"markdown","metadata":{},"source":["# CNN 모델 정의하기"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:42:52.269365Z","iopub.status.busy":"2024-04-06T16:42:52.268741Z","iopub.status.idle":"2024-04-06T16:42:52.280354Z","shell.execute_reply":"2024-04-06T16:42:52.279424Z","shell.execute_reply.started":"2024-04-06T16:42:52.269319Z"},"trusted":true},"outputs":[],"source":["class CNN(torch.nn.Module):\n","\n","    def __init__(self, drop=0.1):\n","        super(CNN, self).__init__()\n","        self.drop = drop\n","        \n","        self.layer1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","            \n","        self.layer2 = torch.nn.Sequential(\n","            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","     \n","        self.dropout = torch.nn.Dropout(p=self.drop)\n","        \n","        # 첫 번째 FC 층\n","        self.fc1 = torch.nn.Linear(7*7*64, 10, bias=True)\n","        torch.nn.init.xavier_uniform_(self.fc1.weight)\n","\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.dropout(out)\n","        out = self.fc1(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["model=CNN().to(device)\n","loss=torch.nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","\n","def model_train(dataloader, model, loss_function, optimizer):\n","\n","    model.train()\n","\n","    train_loss_sum = train_correct = train_total = 0\n","\n","    total_train_batch = len(dataloader)\n","\n","    for images, labels in dataloader:\n","\n","        x_train = images.to(device)\n","        y_train = labels.to(device)\n","\n","        outputs = model(x_train)\n","        loss = loss_function(outputs, y_train)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss_sum += loss.item()\n","\n","        train_total += y_train.size(0)\n","        train_correct += ((torch.argmax(outputs, 1)==y_train)).sum().item()\n","\n","    train_avg_loss = train_loss_sum / total_train_batch\n","    train_avg_accuracy = 100*train_correct / train_total\n","\n","    return (train_avg_loss, train_avg_accuracy)\n","\n","def model_evaluate(dataloader, model, loss_function, optimizer):\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        val_loss_sum = val_correct = val_total = 0\n","\n","        total_val_batch = len(dataloader)\n","\n","        for images, labels in dataloader:\n","\n","            x_val = images.to(device)\n","            y_val = labels.to(device)\n","\n","            outputs = model(x_val)\n","            loss = loss_function(outputs, y_val)\n","\n","            val_loss_sum += loss.item()\n","\n","            val_total += y_val.size(0)\n","            val_correct += ((torch.argmax(outputs, 1)==y_val)).sum().item()\n","\n","        val_avg_loss = val_loss_sum / total_val_batch\n","        val_avg_accuracy = 100*val_correct / val_total\n","\n","    return (val_avg_loss, val_avg_accuracy)\n","     \n","def model_test(dataloader,loss_func, model):\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        test_loss_sum = test_correct = test_total = 0\n","\n","        total_test_batch = len(dataloader)\n","\n","        for images, labels in dataloader:\n","\n","            x_test = images.to(device)\n","            y_test = labels.to(device)\n","\n","            outputs = model(x_test)\n","            loss = loss_func(outputs, y_test)\n","\n","            test_loss_sum += loss.item()\n","\n","            test_total += y_test.size(0)\n","            test_correct += ((torch.argmax(outputs, 1)==y_test)).sum().item()\n","\n","        test_avg_loss = test_loss_sum / total_test_batch\n","        test_avg_accuracy = 100*test_correct / test_total\n","\n","        print('accuracy:', test_avg_accuracy)\n","        print('loss:', test_avg_loss)"]},{"cell_type":"markdown","metadata":{},"source":["# 모델 학습하기"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 01 train acc = 94.174 val acc = 97.400\n","epoch: 02 train acc = 98.037 val acc = 98.267\n","epoch: 03 train acc = 98.556 val acc = 98.267\n","epoch: 04 train acc = 98.772 val acc = 98.600\n","epoch: 05 train acc = 99.024 val acc = 98.417\n","epoch: 06 train acc = 99.172 val acc = 98.683\n","epoch: 07 train acc = 99.157 val acc = 98.633\n","epoch: 08 train acc = 99.367 val acc = 98.783\n","epoch: 09 train acc = 99.417 val acc = 98.933\n","epoch: 10 train acc = 99.485 val acc = 98.933\n","epoch: 11 train acc = 99.539 val acc = 98.750\n","epoch: 12 train acc = 99.619 val acc = 98.800\n","epoch: 13 train acc = 99.574 val acc = 98.667\n","epoch: 14 train acc = 99.715 val acc = 98.767\n","epoch: 15 train acc = 99.694 val acc = 98.967\n","epoch: 16 train acc = 99.702 val acc = 99.100\n","epoch: 17 train acc = 99.724 val acc = 99.000\n","epoch: 18 train acc = 99.750 val acc = 98.783\n","epoch: 19 train acc = 99.757 val acc = 99.017\n","epoch: 20 train acc = 99.756 val acc = 98.967\n"]}],"source":["\n","train_accuracy_list = []\n","val_accuracy_list = []\n","for epoch in range(training_epochs):\n","\n","    train_avg_loss, train_avg_accuracy = model_train(train_dataset_loader, model, loss, optim)\n","    train_accuracy_list.append(train_avg_accuracy)\n","    val_avg_loss, val_avg_accuracy = model_evaluate(validation_dataset_loader, model, loss, optim)\n","    val_accuracy_list.append(val_avg_accuracy)\n","\n","    print('epoch:', '%02d' % (epoch + 1),  'train acc =', '{:.3f}'.format(train_avg_accuracy),    'val acc =', '{:.3f}'.format(val_avg_accuracy))\n"]},{"cell_type":"markdown","metadata":{},"source":["# 예측하기"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy: 99.25\n","loss: 0.031778252591179314\n"]}],"source":["model_test(test_dataset_loader,loss, model)"]},{"cell_type":"markdown","metadata":{},"source":["# 모델 성능\n","\n","## * 최고 정확도\n","- **99.25%**\n","\n","## * 시도한 방법\n","\n","### 데이터 증강\n","- 시도: affine, jitter 등의 데이터 증강\n","- 결과: 손글씨 데이터의 단순성과 이미 충분한 데이터 양 때문에 성능 하락\n","\n","### 학습률 (Learning Rate)\n","- 시도: 0.001, 0.0012, 0.0015\n","- 결과: **0.0012**에서 최고 성능\n","\n","### 드롭아웃 비율 (Drop Rate)\n","- 시도: 0.1, 0.15, 0.2\n","- 결과: **0.1**에서 최고 성능\n","\n","### 배치 크기 (Batch Size)\n","- 시도: 64, 128, 256\n","- 결과: **64**에서 최고 성능\n","\n","### 모델 구조\n","- 시도: 전결합층 추가, 순차적 레이어(conv2d, relu, maxpool2d) 추가\n","- 결과: 성능 향상 없음\n","\n","### 최적화 알고리즘 (Optimizer)\n","- 시도: Adam, AdamW\n","- 결과: **Adam**에서 근소한 차이로 더 높은 성능\n","\n","---\n","\n","이러한 실험을 통해, 학습률, 드롭아웃 비율, 배치 크기의 최적값을 찾아내고, 데이터 증강과 모델 구조 변화는 본 데이터셋에 대해 큰 영향을 미치지 않는다는 결론을 내렸습니다. 최적화 알고리즘으로는 Adam이 AdamW보다 약간 더 나은 성능을 보였습니다.\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
